----
authors: Pedro Palaz√≥n Candel <pedro@joyent.com>, Trent Mick <trent@joyent.com>
contributors: Robert Mustacchi <rm@joyent.com>, Joshua Clulow <jclulow@joyent.com>
state: draft
----

<!--
    This Source Code Form is subject to the terms of the Mozilla Public
    License, v. 2.0. If a copy of the MPL was not distributed with this
    file, You can obtain one at http://mozilla.org/MPL/2.0/.
-->

<!--
    Copyright 2015 Joyent Inc.
-->

# RFD 3 SDC Compute Nodes Reboot

## Introduction

A new `sdcadm experimental reboots` command (unpromised interface) for working
towards controlled and safe reboots of selected servers in an SDC.

One of the least specified and hardest parts of SDC upgrades right now is
managing the reboots of CNs and the headnode safely. In particular:

- controlling reboots of the "core" servers (those with SDC core components,
  esp. the HA binders and manatees)
- reasonably helpful tooling for rebooting (subsets of) the other servers in a
  DC: rolling reboots, reboot rates

While the desired command interface is pretty similar to other sdcadm
subcommands, like`sdcadm platform assign`, including pretty much the same
common options, and the specific server selection options related to the
aforementioned constraints:


    -r INT, --rate=INT  Number of servers to reboot simultaneously

    -c, --core          Reboot the servers with SDC core components
    -o, --non-core      Reboot the servers without SDC core components
    -a, --all           Reboot all the servers

What it's more complex and the main reason for this RFD is the way to proceed
with the reboot process of the selected Compute Nodes.

## Reboot plan

It has been proposed to use a **"reboot plan"** similar to the _upgrade
plans_ already generated by `sdcadm` and, instead of being executed by the
`sdcadm` process, the reboot plan should live and the execution should be
fired by CNAPI.

The following is the proposed reboot plan information to be sent to CNAPI
on a single HTTP request:

- name of reboot plan
- start time for reboot plan (could be immediate)
- list of servers to reboot (uuids)
- maximum number of servers that can be offline at once
- maximum time each server should be offline before an alarm is triggered

This HTTP request should return a UUID from CNAPI that identifies the reboot
plan as it's stored. CNAPI should then act on the reboot plan as specified,
handling service interruptions and daemon restarts as gracefully as possible.

`sdcadm` can query the list of current or future reboot plans, and monitor
the live progress of a current reboot plan (including the one the operator
just started)

The reboot plan should also store historical data about how long each server
was offline, so that we can make better estimates of downtime in future plans.


## Constraints

### Reboot of CNs hosting core components

If we decide that the reboot plan should also handle reboots of the "core"
servers, we'll need to add a bunch of `manatee-adm` related logic - already
present at `sdcadm` code base - to CNAPI.

CNAPI itself is _manatee's dependent_ and trying to handle reboots of the
CNs hosting manatee's shard members could give us problems. In fact, the way
we use to check for a successful reboot of core CNs is checking the status
of the manatee shard from one of the available shard members, while we check
the availability of the non-core CNs rebooted using CNAPI (and manatee).

A possible workaround for this problem could be to split the reboot process
in two parts:

1. Reboot CNs with core components
2. Create a reboot plan just for the non-core CNs selected for reboot

CNs with core components need to be rebooted before any other CNs anyway, and
using a sequence stablished by manatee's shard administration. The main
problem would be to keep track of the reboot of these core CNs in terms of
how long it took to reboot each one of these nodes and, of course, if we
drive such CNs from the `sdcadm` process itself, we will not be able to
_schedule_ reboot of CNs hosting core SDC components. Which I'd say has sense,
since a failure in the reboot of these CNs will likely compromise the state
of the whole SDC setup and, therefore, should be an attended operation.

Input welcome.

### CNAPI Schedule and execution of the reboot plan

At the moment, I don't think CNAPI has any code useful to deal with
schedule of the execution of a reboot plan or anything else. It's merely
the HTTP server process.

In order to make possible to make a choice on the execution time of a reboot
plan, we could possibly use node-workflow, and create a job with the servers
to be rebooted. There would be a workflow, which should be used by CNAPI in
order to _"queue"_ the reboot of the selected servers. It would receive the
servers to be rebooted and the desired time to begin with the plan execution
as arguments. It could even poll the servers for successful reboot and
eventually fire an alarm when the reboot takes too long.

Using job's `exec_after` property we could decide when this job would have
the green light to be executed - assuming there are available wf-runners -
without having to implement anything specific for CNAPI.

We could also use a combination of `job.locks` and `job.target` properties
to decide when an exising instance of this _"reboot CNs"_ jobs should allow
or not the creation of more jobs of the same type.

Thoughts?.


### Reboots concurrency/rate

We need to implement a way - in the workflow, if we choose the approach above -
to limit the concurrency of the servers being rebooted at the same time.

We also need something similar for `sdcadm platform assign` and possibly other
`sdcadm` subcommands (agents updates in both, shar and individual flavors).

Would be worth considering to implement something simmilar to Josh Clulow's
RunQueue (https://github.com/joyent/node-urclient/blob/master/lib/runqueue.js)
into a separated module, so we can use it from wherever it is needed?


### Failure handling

A failure on the reboot of one or more of the nodes involved into a reboot
plan shouldn't make the whole plan to fail, but just to report that the
reboot of those nodes failed.

In my opinion, the reboot plan shouldn't attempt any retries on the CNs
reboots and, once the failures of the reboot of the nodes is reported clearly,
it should move into the next CN to be rebooted, until it is done with the whole
list of nodes included into the plan.

But, maybe a reasonable approach to reboot retries, or a more opinionated
explanation of why retries are a bad idea could be helpful here.

## Affected APIs.


### Moray/Data Storage

At the moment, if we pick CNAPI as the application in charge to execute the
reboot plans, it looks like the natural way to store reboot plans to be
executed and for historical purposes is moray. Anyway, this is a good time
to think about this "default choice" and figure out if this may impact general
CNAPI performance for other end-points or, eventually, if we may need an
additional/different storage.

### CNAPI:

- New end-points for reboot plans + new workflow to execute such plans.
- End-points to query progress of the reboot plan's associated job.
- New model reboot plan (or whatever the name we come with) + new bucket?

### Workflow:

- If we need a concurrency related library, we'll need to update node modules
for sdc-workflow.

### sdcadm

- New subcommand
- Tools for dealing with CNs hosting manatee shard members
- Generation of reboot plans
- Tools for polling a reboot plan (being or not executed). Do we need a way
to list _"queued"_ reboot plans? All of them?, just pending ones?, ...

The following could be the command line interface for the main `reboots`
subcommand - the one creating the reboot plans - which is also open to discuss:


    Controlled and safe reboots of selected servers in an SDC.

    Note that this command will not reboot the headnode.

    Usage:
         sdcadm experimental reboots [ -a | -c | -o ] [ -h ] [-n ] [ -y ]
                      [SERVER] [SERVER]...

    A "SERVER" is a server UUID or hostname. In a larger datacenter, getting
    a list of the wanted servers can be a chore. The "sdc-server lookup ..."
    tool is useful for this.

    Examples:
        # Reboot the core servers servers.
        sdcadm experimental reboots --core

        # Reboot non core servers with the "pkg=aegean" trait.
        sdcadm experimental reboots --non-core \
            $(sdc-server lookup setup=true traits.pkg=aegean | xargs)
        # Reboot non core servers, excluding those with a "internal=PKGSRC" trait.
        sdcadm experimental reboots --non-core \
            $(sdc-server lookup setup=true 'traits.internal!~PKGSRC' | xargs)

    Options:
        -y, --yes                    Answer yes to all confirmations.
        -h, --help                   Show this help.
        -n, --dry-run                Go through the motions without actually
                                     rebooting.
        -r INT, --rate=INT           Number of servers to reboot
                                     simultaneously. Default: 5.

      Server selection:
        -c, --core                   Reboot the servers with SDC core
                                     components, (but the Headnode).
        -o, --non-core               Reboot the servers without SDC core
                                     components.
        -a, --all                    Reboot all the servers (but the Headnode).

